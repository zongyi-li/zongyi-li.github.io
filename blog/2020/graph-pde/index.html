<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Zongyi Li | Graph Neural Operator for PDEs</title>
  <meta name="description" content="Zongyi's personal website.
">

  

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/blog/2020/graph-pde/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Zongyi</strong> Li
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">about</a>

        <!-- Blog -->
        <a class="page-link" href="/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="/publications/">Publications</a>
          
        
          
            <a class="page-link" href="/teaching/">Teaching</a>
          
        
          
        
          
            <a class="page-link" href="/_pages/projects/"></a>
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Graph Neural Operator for PDEs</h1>
    <p class="post-meta">April 8, 2020</p>
  </header>

  <article class="post-content">
    <blockquote>
  <p>The blog takes about 10 minutes to read.
It introduces our recent work that uses graph neural networks to learn 
<strong>mappings between function spaces</strong> and solve partial differential equations. 
You can also check out the <a href="https://arxiv.org/abs/2003.03485">paper</a>
and <a href="https://github.com/zongyi-li/graph-pde">code</a> for more formal derivations.
<!--- Background of graph neural networks (GNN) and partial differential equations (PDE) will be helpful.
---></p>
</blockquote>

<h3 id="introduction">Introduction</h3>

<p>Scientific computations are expensive. 
It could take days and months for numerical solvers to simulate fluid dynamics and many-body motions. 
Because to achieve good accuracy, 
the numerical solvers need to discretize the space and time into very fine grids 
and solve a great number of equations on the grids.
Recently, people are developing data-driven methods based on machine learning techniques such as deep learning.
Instead of directly solving the problems, data-driven solvers learn from the data of the problems and solutions.
When querying new problems, data-driven solvers can directly give predictions based on the data.
Since they don’t need to discretize the space into very small pieces and solve all these equations,
these data-driven solvers are usually much faster compared to traditional numerical solvers,</p>

<p>However, data-driven solvers are subject to the quality of the data given.
If the training data is not good enough, they can’t make good predictions.
In scientific computing, usually, the training data are generated by the traditional numerical solvers.
And to generate good enough data, it still takes days and months for these numerical solvers.
Sometime, data are observed from experiments and there are just no good training data.
Especially, people consider neural networks as interpolators which may not be able to extrapolate.
It is unclear whether neural networks can generalize to unseen data. 
So if the training data are of one resolution, 
the learned solvers can only solve the problem in this specific resolution. 
In general, generalization is a crucial problem in machine learning.
It becomes a trade-off: these machine learning based methods make evaluation easier, 
but the training process could be even more painful.</p>

<p>To dealing with this problem, we purpose operator learning. By encoding certain structures,
we let the neural network learn the mapping of functions and generalize among different resolutions.
As a result, we can first use a numerical method generated some less-accurate, low-resolution data, 
but the learned solver is still able to give reasonable, high-resolution predictions.
In some sense, both training and evaluation can be pain-free.</p>

<h3 id="operator-learning">Operator Learning</h3>

<p>In mathematics, <a href="https://en.wikipedia.org/wiki/Operator_(mathematics\)">operators</a> 
are usually referring to the mappings between function spaces. 
You most likely have already encountered some operators.
For example, the differentiation and integration are operators. 
When we take the derivative  or do an indefinite integration of a function, 
we will get another function. 
In other words, the differentiation and integration are mappings from function space to function space.</p>

<p>In scientific computing, usually the problem is to solve some form of differential equations
(<a href="https://en.wikipedia.org/wiki/Partial_differential_equation">PDEs</a>).
Consider a general differential equation of the form:</p>

<script type="math/tex; mode=display">\mathcal{L}u = f</script>

<p>where  <script type="math/tex">u</script> and <script type="math/tex">f</script> are some functions on the physical domain, and
<script type="math/tex">\mathcal{L}</script> is some differential operator that maps 
the function <script type="math/tex">u</script> to the function <script type="math/tex">f</script>. 
Usually, <script type="math/tex">\mathcal{L}</script> and <script type="math/tex">f</script> are given. The task is to solve for <script type="math/tex">u</script>.
That is, we want to learn an operator like the inverse of <script type="math/tex">\mathcal{D}</script> that 
maps the function <script type="math/tex">f</script> to the function <script type="math/tex">u</script>. 
So the problem of PDE is indeed an operator learning problem.</p>

<p>The classical development of neural networks has been primarily 
for mappings between a finite-dimensional Euclidean space and a set of classes
(e.g. an image vector to a label), 
or between two finite-dimensional Euclidean spaces (e.g. an image vector to another image vector).
However, many problems in physics and math involve learning the mapping between function spaces,
which poses a limitation on the classical neural network based methods.
Besides all these problem governed by differential equations, 
we are learning operators in many common machine learning setting. 
For a bold example, images should be considered as functions of light defined on a continuous region, 
instead of as <script type="math/tex">32 \times 32</script> pixel vectors.</p>

<p>In this work, we aim to generalize neural networks so that they can learn operators, 
the mappings between infinite-dimensional spaces, with a special focus on PDEs.</p>

<h3 id="limitation-of-fixed-discretization">Limitation of Fixed Discretization</h3>

<p>PDEs are, unfortunately, hard. 
Instead of learning the operator, people usually discretize the physical domain
and cast the problem in finite-dimensional Euclidean space.
Indeed, hundred years of effort has been made to develop numerical solvers 
such as the finite element method and finite difference method.</p>

<div class="img_row">
    <img class="col three" src="/assets/img/grids.png" alt="" title="Discretizations" />
</div>
<div class="col three caption">
Three examples of discretization. 
The left one is a regular grid used in the finite difference method;
the middle one is a triangulated grid used in the finite element method;
the right one is a cylinder mesh for real-world airfoil problem.
</div>

<p>Just like how we store images by pixels in <em>.PNG</em> and <em>.JPG</em> formats, 
we need to discretize the domain of PDEs into some grid and solve the equation on the grid.
It really makes the thing easier.
These traditional numerical solvers are awesome, but they have some drawbacks:</p>

<ul>
  <li>The error scales steeply with the resolution. 
We need a high resolution to get good approximations.</li>
  <li>The computation and storage steeply scale with the resolution (i.e. the size of the grid).</li>
  <li>When the equation is solved on one discretization, 
we cannot change the discretization anymore.</li>
</ul>

<p><em>.PNG</em> and <em>.JPG</em> formats are good. 
But sometimes maybe we want to save the images as vector images in <em>.EPS</em> or <em>.SVG</em> formats,
so that it can be used and displayed in any context. 
And for some images, the vector image format is more convenient and efficient.
Similarly, we want to find the continuous version for PDEs, an operator that is invariant of discretization.</p>

<p>Furthermore, mathematically speaking, such continuous, 
discretization-invariant format is in some sense, closer to the real, analytic solution. 
It has an important mathematical meaning. 
Bear the motivation in mind. Let’s develop a rigorous formulation.</p>

<h3 id="problem-setting">Problem Setting</h3>

<p>Let’s be more concrete. Consider the standard second order elliptic PDE</p>

<script type="math/tex; mode=display">- \nabla \cdot (a(x) \nabla u(x))  = f(x), \quad  x \in D</script>

<script type="math/tex; mode=display">u(x) = 0, \quad x \in \partial D</script>

<p>for some bounded, open domain <script type="math/tex">D \subset \mathbb{R}^d</script> and a fixed source function
<script type="math/tex">f</script>. This equation is prototypical of PDEs arising in
numerous applications including hydrology  and elasticity. 
For a given function <script type="math/tex">a</script>, 
the equation has a unique weak solution <script type="math/tex">u</script> 
and therefore we can define the solution operator <script type="math/tex">\mathcal{F}_{true}</script>
as the map from function to function <script type="math/tex">a \mapsto u</script>.</p>

<p>Our goal is to learn a operator <script type="math/tex">\mathcal{F}</script> approximating <script type="math/tex">\mathcal{F}_{true}</script>, 
by using a finite collection of observations of input-output pairs 
<script type="math/tex">\{a_j, u_j\}_{j=1}^N</script>, where each <script type="math/tex">a_j</script> and <script type="math/tex">u_j</script> are functions on <script type="math/tex">D</script>.
In practice, the training data is solved numerically or observed in experiments.
In other words, functions <script type="math/tex">a_j</script> and <script type="math/tex">u_j</script> come with discretization.<br />
Let <script type="math/tex">P_K = \{x_1,\dots,x_K\} \subset D</script> be a <script type="math/tex">K</script>-point discretization of the domain
<script type="math/tex">D</script> and assume we have observations <script type="math/tex">a_j|_{P_K}, u_j|_{P_K}</script>, for a finite
collection  of input-output pairs indexed by <script type="math/tex">j</script>. 
We will show how to learn a discretization-invariant mapping based on discretized data.</p>

<h3 id="kernel-formulation">Kernel Formulation</h3>

<p>For a general PDE of the form:</p>

<script type="math/tex; mode=display">(\mathcal{L}_a u)(x)= f(x), \quad x \in D</script>

<script type="math/tex; mode=display">u(x) = 0, \quad x \in \partial D</script>

<p>Under fairly general conditions on <script type="math/tex">\mathcal{L}_a</script>, 
we may define the Green’s function <script type="math/tex">G : D \times D \to \mathbb{R}</script> as the 
unique solution to the problem</p>

<script type="math/tex; mode=display">\mathcal{L}_a G(x, \cdot) = \delta_x</script>

<p>where <script type="math/tex">\delta_x</script> is the delta measure on <script type="math/tex">\mathbb{R}^d</script> centered at <script type="math/tex">x</script>. 
Note that <script type="math/tex">G</script> will depend on the coefficient <script type="math/tex">a</script> thus we will henceforth denote it as <script type="math/tex">G_a</script>. 
Then operator <script type="math/tex">\mathcal{F}_{true}</script> can be written as an integral operator of green function:</p>

<script type="math/tex; mode=display">u(x) = \int_D G_a(x,y)f(y) \: dy</script>

<p>Generally the Green’s function is continuous at points <script type="math/tex">x \neq y</script>, 
for example, when <script type="math/tex">\mathcal{L}_a</script> is uniformly elliptic. 
Hence it is natural to model the kernel via a neural network <script type="math/tex">\kappa</script>. 
Just as the Green function, the kernel network <script type="math/tex">\kappa</script> takes input <script type="math/tex">(x,y)</script>.
Since the kernel depends on <script type="math/tex">a</script>, we let <script type="math/tex">\kappa</script> also take input <script type="math/tex">(a(x),a(y))</script>.</p>

<script type="math/tex; mode=display">u(x) = \int_D \kappa(x,y,a(x),a(y))f(y) \: dy</script>

<h3 id="as-an-iterative-solver">As an Iterative Solver</h3>

<p>In our setting, <script type="math/tex">f</script> is an unknown but fixed function. 
Instead of doing the kernel convolution with <script type="math/tex">f</script>, 
we will formulate it as an iterative solver 
that approximated <script type="math/tex">u</script> by <script type="math/tex">u_t</script>,
where <script type="math/tex">t = 0,\ldots,T</script> is the time step.</p>

<p>The algorithm starts from an initialization <script type="math/tex">u_0</script>, 
for which we use <script type="math/tex">u_0(x) = (x, a(x))</script>.
At each time step <script type="math/tex">t</script>, it updates <script type="math/tex">u_{t+1}</script> by an kernel convolution of <script type="math/tex">u_{t}</script>.</p>

<script type="math/tex; mode=display">u_{t+1}(x) = \int_D \kappa(x,y,a(x),a(y))u_{t}(x) \: dy</script>

<p>It works like an implicit iteration. 
At each iteration the algorithm solves an equation of <script type="math/tex">u_{t}(x)</script> and <script type="math/tex">u_{t+1}(x)</script>
by the kernel integral. <script type="math/tex">u_T</script> will be output as the final prediction.</p>

<p>To further take the advantage of neural networks, 
we will lift <script type="math/tex">u(x) \in \mathbb{R}^d</script> 
to a high dimensional representation <script type="math/tex">v(x) \in \mathbb{R}^n</script>,
with <script type="math/tex">n</script> the dimension of the hidden representation.</p>

<p>The overall algorithmic framework follow:</p>

<script type="math/tex; mode=display">v_0(x) = NN_1 (x, a(x))</script>

<script type="math/tex; mode=display">v_{t+1}(x) = \sigma\Big( W v_t(x)  
+ \int_{B(x,r)} \!\!\!\kappa_{\phi}\big(x,y,a(x),a(y)\big)
v_t(y)\: \mathrm{d}y \Big)
\quad \text{for } \ t=1,\ldots,T</script>

<script type="math/tex; mode=display">u(x) = NN_2 (v_T (x))</script>

<p>where <script type="math/tex">NN_1</script> and <script type="math/tex">NN_2</script> are two feed-forward neural networks 
that lifts the initialization to hidden representation <script type="math/tex">v</script>
and projects the representation back to the solution <script type="math/tex">u</script>, respective.
<script type="math/tex">\sigma</script> is an activation function such as ReLU.
the additional term <script type="math/tex">W \in \mathbb{R}^{n \times n}</script> is a linear transformation 
that acts on $v$.
Notice, since the kernel integration happens in the high dimensional representation,
the output of <script type="math/tex">\kappa_{\phi}</script> is not a scalar, 
but a linear transformation <script type="math/tex">\kappa_{\phi}\big(x,y,a(x),a(y)\big)\in \mathbb{R}^{n \times n}</script>.</p>

<h3 id="graph-neural-networks">Graph Neural Networks</h3>

<p>To do the integration, we again need some discretization.<br />
Assuming a uniform distribution of <script type="math/tex">y</script>, 
the integral <script type="math/tex">\int_{B(x,r)} \kappa_{\phi}\big(x,y,a(x),a(y)\big)
v_t(y)\: \mathrm{d}y</script> can be approximated by a sum:</p>

<script type="math/tex; mode=display">\frac{1}{|N|}\sum_{y \in N(x)} \kappa(x,y,a(x),a(y))v_t(y)</script>

<blockquote>
  <p>Observation: the kernel integral is equivalent to the message passing on graphs</p>
</blockquote>

<p>If you are similar with graph neural network, 
you may have already realized this formulation is the same as 
the aggregation of messages in graph network.
Message passing graph networks comprise a standard architecture employing edge features 
(gilmer et al, 2017).</p>

<p>If we properly construct graphs on the spatial domain <script type="math/tex">D</script> of the PDE, 
the kernel integration can be viewed as an aggregation of messages.
Given node features <script type="math/tex">v_t(x) \in \mathbb{R}^{n}</script>, 
edge features <script type="math/tex">e(x,y) \in \mathbb{R}^{n_e}</script>, 
and a graph <script type="math/tex">G</script>, the message passing neural network with averaging aggregation is</p>

<script type="math/tex; mode=display">v_{t+1}(x) =  \sigma\Big(W v_t(x) + 
\frac{1}{|N(x)|} \sum_{y \in N(x)} \!\kappa_{\phi}\big(e(x,y)\big) v_t(y)\Big)</script>

<p>where <script type="math/tex">W \in \mathbb{R}^{n \times n}</script>, 
<script type="math/tex">N(x)</script> is the neighborhood of <script type="math/tex">x</script> according to the graph, 
<script type="math/tex">\kappa_{\phi}\big(e(x,y)\big)</script> is a neural network 
taking as input edge features and as output 
a matrix in <script type="math/tex">\mathbb{R}^{n \times n}</script>. 
Relating to our kernel formulation, <script type="math/tex">e(x,y) = (x,y,a(x),a(y))</script>.</p>

<div class="img_row">
    <img class="col three" src="/assets/img/graph.png" title="Graph on the domain" />
</div>
<div class="col three caption">
</div>

<h3 id="nystrom-approximation">Nystrom Approximation</h3>
<p>Ideally, to use all the information available, 
we should construct <script type="math/tex">K</script> nodes in the graph for all the points in the discretization
<script type="math/tex">P_k = \{x_1,\ldots, x_K\}</script>, which will create <script type="math/tex">O(K^2)</script> edges. 
It is quite expensive.
Thankfully, we don’t need all the points to get an accurate approximation.
For each graph, the error of Monte Carlo approximation of the kernel integral
<script type="math/tex">\int_{B(x,r)} \kappa_{\phi}(x,y) v_t(y)\: \mathrm{d}y</script> scales with <script type="math/tex">m^{-1/2}</script>, 
where <script type="math/tex">m</script> is the number of nodes sampled.</p>

<p>Since we will sample <script type="math/tex">N</script> graphs in total for all <script type="math/tex">N</script> training examples <script type="math/tex">\{a_j, u_j\}^N</script>, 
the overall error of the kernel is much smaller than <script type="math/tex">m^{-1/2}</script>.
In practice, sampling <script type="math/tex">m \sim 200</script> nodes is sufficient for <script type="math/tex">K \sim 100000</script> points.</p>

<p>It is possible to further improve the approximation 
using more sophisticated Nystrom Approximation methods. 
For example, we can estimate the importance of each points, 
and add more nodes to the difficult and singularity areas in the PDEs.</p>

<h3 id="experiments-poisson-equations">Experiments: Poisson Equations</h3>
<p>First let’s do a sanity check. Consider a simple poisson equation:</p>

<script type="math/tex; mode=display">-\Delta u = f</script>

<p>We set <script type="math/tex">v_0 = f</script> and <script type="math/tex">T=1</script>, using one iteration of the graph kernel network 
to learn the operator <script type="math/tex">\mathcal{F}: f \mapsto u</script>.</p>

<h5 id="1d-poisson-equation"><script type="math/tex">1D</script> poisson equation</h5>

<div class="img_row">
    <img class="col three" src="/assets/img/nik_kernel.png" title="Kernel of 1d Poisson" />
</div>
<div class="col three caption">
</div>

<p>As shown in the figure above, we compare the true analytic Green function <script type="math/tex">G(x,y)</script> (left) 
with the learned kernel <script type="math/tex">\kappa_{\phi}(x,y)</script>  (right). 
The learned kernel is almost the same as the true kernel, 
which means are neural network formulation does match the Green function expression.</p>

<h5 id="2d-poisson-equation"><script type="math/tex">2D</script> poisson equation</h5>

<div class="img_row">
    <img class="col three" src="/assets/img/GKN_compare.png" title="2d Poisson" />
</div>
<div class="col three caption">
</div>

<p>By assuming the kernel structure, 
graph kernel networks need only a few training examples to learn the shape of solution <script type="math/tex">u</script>.
As shown in the figure above, the graph kernel network can roughly learn <script type="math/tex">u</script> with <script type="math/tex">5</script> training pairs,
which a feedforward neural network need at least <script type="math/tex">100</script> training examples.</p>

<h3 id="experiments-generalization-of-resolution">Experiments: generalization of resolution</h3>

<p>For the large scale experiments, we use Darcy equation of the form</p>

<script type="math/tex; mode=display">- \nabla \cdot (a(x) \nabla u(x))  = f(x), \quad  x \in D</script>

<script type="math/tex; mode=display">u(x) = 0, \quad x \in \partial D</script>

<p>and learn the operator <script type="math/tex">\mathcal{F}: a \mapsto u</script>.</p>

<p>To show the generalization property, we train the graph kernel network 
with nodes sampled from the resolution <script type="math/tex">s \times s</script> 
and test on another resolution <script type="math/tex">s' \times s'</script> .</p>

<table>
  <thead>
    <tr>
      <th>Resolutions</th>
      <th style="text-align: left">s’=61</th>
      <th style="text-align: left">s’=121</th>
      <th style="text-align: left">s’=241</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>s=16</td>
      <td style="text-align: left">0.0717</td>
      <td style="text-align: left">0.0768</td>
      <td style="text-align: left">0.0724</td>
    </tr>
    <tr>
      <td>s=31</td>
      <td style="text-align: left">0.0726</td>
      <td style="text-align: left">0.0710</td>
      <td style="text-align: left">0.0722</td>
    </tr>
    <tr>
      <td>s=61</td>
      <td style="text-align: left">0.0687</td>
      <td style="text-align: left">0.0728</td>
      <td style="text-align: left">0.0723</td>
    </tr>
    <tr>
      <td>s=121</td>
      <td style="text-align: left">0.0687</td>
      <td style="text-align: left">0.0664</td>
      <td style="text-align: left">0.0685</td>
    </tr>
    <tr>
      <td>s=241</td>
      <td style="text-align: left">0.0649</td>
      <td style="text-align: left">0.0658</td>
      <td style="text-align: left">0.0649</td>
    </tr>
  </tbody>
</table>

<p>As shown in the table above for each row, 
the test errors on different resolutions are about the same, 
which means the graph kernel network can
also generalize in the semi-supervised setting. 
An figure for <script type="math/tex">s=16, s'=241</script> is following (where error is absolute squared error):</p>

<div class="img_row">
    <img class="col three" src="/assets/img/uai_16to241.png" title="generalization of resolution" />
</div>
<div class="col three caption">
</div>

<h3 id="experiments-comparision-of-benchmarks">Experiments: Comparision of Benchmarks</h3>

<p>Finally, we compare graph network with different benchmarks on Darcy equation 
in the fixed discretization setting where the resolution <script type="math/tex">s=421</script>.</p>

<table>
  <thead>
    <tr>
      <th>Networks</th>
      <th style="text-align: left">s=85</th>
      <th style="text-align: left">s’=141</th>
      <th style="text-align: left">s’=211</th>
      <th style="text-align: left">s’=421</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>NN</td>
      <td style="text-align: left">0.1716</td>
      <td style="text-align: left">0.1716</td>
      <td style="text-align: left">0.1716</td>
      <td style="text-align: left">0.1716</td>
    </tr>
    <tr>
      <td>FCN</td>
      <td style="text-align: left">0.0253</td>
      <td style="text-align: left">0.0493</td>
      <td style="text-align: left">0.0727</td>
      <td style="text-align: left">0.1097</td>
    </tr>
    <tr>
      <td>PCA+NN</td>
      <td style="text-align: left">0.0299</td>
      <td style="text-align: left">0.0298</td>
      <td style="text-align: left">0.0298</td>
      <td style="text-align: left">0.0299</td>
    </tr>
    <tr>
      <td>RBM</td>
      <td style="text-align: left">0.0244</td>
      <td style="text-align: left">0.0251</td>
      <td style="text-align: left">0.0255</td>
      <td style="text-align: left">0.0259</td>
    </tr>
    <tr>
      <td>GKN</td>
      <td style="text-align: left">0.0346</td>
      <td style="text-align: left">0.0332</td>
      <td style="text-align: left">0.0342</td>
      <td style="text-align: left">0.0369</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <p>NN is a simple point-wise feedforward neural network. 
It is mesh-free, but performs badly due to lack of neighbor information.</p>
  </li>
  <li>
    <p>FCN is the state of the art neural network method based on Fully Convolution Network[ZhuandZabaras,2018]. 
It has a dominating performance for small grids <script type="math/tex">s = 61</script>. 
But fully convolution networks are mesh-dependent 
and therefore their error grows when moving to a larger grid.
Besides the scaling of computation, one needs to work hard on parameter-tuning for each specific resolution.</p>
  </li>
  <li>
    <p>PCA+NN is an instantiation of the methodology proposed in [Bhattacharya et al., 2020]: 
using PCA as an auto-encoder on both the input and output data 
and interpolating the latent spaces with a neural network. 
The method provably obtains mesh-independent error and can learn purely from data, 
however the solution can only be evaluated on the same mesh as the training data.</p>
  </li>
  <li>
    <p>RBM is the classical Reduced Basis Method (using a PCA basis), 
which is widely used in applications and provably obtains mesh-independent error [DeVore, 2014]. 
It has the best performance but the solutions can only be evaluated 
on the same mesh as the training data and one needs knowledge of the PDE to employ it.</p>
  </li>
  <li>
    <p>GKN stands for our graph kernel network. 
It enjoys competitive performance against all other methods 
while being able to generalize to different mesh geometries.</p>
  </li>
</ul>

<h3 id="conclusion">Conclusion</h3>

<p>In the work we purposed to use graph networks for operator learning in PDE problems. 
By varying the underlying graph and discretization, 
the learned kernel is invariant of the discretization. 
Experiments confirm the graph kernel networks are able to generalize among different discretization.
And in the fixed discretization setting, the graph kernel networks also have good performances compared to several benchmark.</p>


  </article>

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2021 Zongyi Li.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">


<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>



  </body>

</html>
